#!/usr/bin/env python

import os
import numpy as np
import torch
from gradslam.slam.pointfusion import PointFusion
from gradslam.slam.icpslam import ICPSLAM
from gradslam import Pointclouds, RGBDImages
from supervised_depth_correction.data import Dataset
from supervised_depth_correction.utils import load_model
from supervised_depth_correction.metrics import localization_accuracy
from supervised_depth_correction.transform import translation_norm, rotation_angle, delta_transform
from supervised_depth_correction.postprocessing import filter_depth_outliers
# ROS
import rospy
from sensor_msgs.msg import CameraInfo, PointCloud2
from nav_msgs.msg import Path
from geometry_msgs.msg import PoseStamped, TransformStamped
from ros_numpy import msgify
from tf.transformations import quaternion_from_matrix
import tf2_ros


def publish_tf_pose(pose, child_frame_id):
    assert isinstance(pose, PoseStamped)
    br = tf2_ros.TransformBroadcaster()
    t = TransformStamped()
    t.header.stamp = pose.header.stamp
    t.header.frame_id = pose.header.frame_id
    t.child_frame_id = child_frame_id
    t.transform.translation.x = pose.pose.position.x
    t.transform.translation.y = pose.pose.position.y
    t.transform.translation.z = pose.pose.position.z
    t.transform.rotation.x = pose.pose.orientation.x
    t.transform.rotation.y = pose.pose.orientation.y
    t.transform.rotation.z = pose.pose.orientation.z
    t.transform.rotation.w = pose.pose.orientation.w
    br.sendTransform(t)


def matrix_to_pose_msg(T, frame_id='world'):
    assert T.shape == (4, 4)
    pose = PoseStamped()
    pose.header.frame_id = frame_id
    pose.header.stamp = rospy.Time.now()
    pose.pose.position.x = T[0, 3]
    pose.pose.position.y = T[1, 3]
    pose.pose.position.z = T[2, 3]
    q = quaternion_from_matrix(T)
    pose.pose.orientation.x = q[0]
    pose.pose.orientation.y = q[1]
    pose.pose.orientation.z = q[2]
    pose.pose.orientation.w = q[3]
    return pose


class GradslamROS:
    def __init__(self, subseq: str = ''):
        self.device = torch.device(rospy.get_param('~device', 'cuda:0'))
        # self.slam = PointFusion(odom=rospy.get_param('~odometry', 'gt'), dsratio=1, device=self.device)
        self.slam = ICPSLAM(odom=rospy.get_param('~odometry', 'gradicp'), dsratio=1, device=self.device)
        self.pointclouds = Pointclouds(device=self.device)
        self.current_pose = None
        self.rgbd_img = None
        self.world_frame = 'world'
        self.camera_frame = 'camera'
        self.path = Path()
        self.path.header.frame_id = self.world_frame
        self.path_gt = Path()
        self.path_gt.header.frame_id = self.world_frame
        self.path_len = 0.0
        self.path_pub = rospy.Publisher('~path', Path, queue_size=2)
        self.path_gt_pub = rospy.Publisher('~path_gt', Path, queue_size=2)
        self.pc_pub = rospy.Publisher('~cloud', PointCloud2, queue_size=1)
        self.caminfo_pub = rospy.Publisher('~frustum', CameraInfo, queue_size=1)
        self.map_step = rospy.get_param('~map_step', 4)
        self.camera = rospy.get_param('~camera', 'left')  # 'right', 'left'
        self.ds = Dataset(subseq,
                          depth_type=rospy.get_param('~depth_type', "sparse"),
                          camera=self.camera,
                          zero_origin=False,
                          device=self.device)
        self.delay_start = rospy.get_param('~delay_start', 1.0)
        self.period = rospy.get_param('~period', 1.0)
        self.index = 0
        self.ds_step = 1
        self.depth_completion = rospy.get_param('~depth_completion', False)
        self.depth_filterring = rospy.get_param('~depth_filterring', False)
        self.dc_model = load_model(rospy.get_param('~dc_model_path',
                                                   os.path.realpath(os.path.join(os.path.dirname(__file__), '../config/results/weights/weights-539.pth'))))
        self.dc_model.to(self.device)
        self.dc_model.eval()
        self.poses_gt = list(self.ds.get_gt_poses().squeeze().to(self.device))
        self.poses = list()
        self.timer = rospy.Timer(rospy.Duration(self.delay_start), self.start_timer, oneshot=True)

    def start_timer(self, evt):
        self.timer = rospy.Timer(rospy.Duration(self.period), self.run)
        rospy.loginfo('Publishing robot data.')

    def move(self):
        if self.index < len(self.ds):
            self.index += 1
            rospy.logdebug('Moved to next data sample')

    def slam_step(self):
        colors, depths, intrinsics, poses = self.ds[self.index * self.ds_step]
        if self.depth_filterring:
            depths = filter_depth_outliers(depths, min_depth=2.0, max_depth=15.0)

        # depth completion model forward pass
        if self.dc_model is not None and self.depth_completion:
            rospy.loginfo('Applying depth completion model')
            with torch.no_grad():
                depths = self.dc_model(depths, mask=(depths > 0).float())

        rgbd_img = RGBDImages(colors, depths, intrinsics, poses)
        self.pointclouds, rgbd_img.poses = self.slam.step(self.pointclouds, rgbd_img, self.rgbd_img)

        self.rgbd_img = rgbd_img
        self.current_pose = rgbd_img.poses.squeeze()
        self.poses.append(self.current_pose)
        rospy.logdebug(f'Point cloud shape: {self.pointclouds.points_list[0].shape}')

    def compute_localization_accuracy(self):
        # trans_delta, rot_delta = localization_accuracy(self.poses_gt[:len(self.poses)],
        #                                                self.poses, trans_rot_combined=False)
        # rospy.loginfo("Localization accuracy, position: %.3f [m]" % trans_delta)
        # rospy.loginfo("Localization accuracy, orientation: %.3f [rad]" % rot_delta)

        delta_pose = delta_transform(self.poses_gt[len(self.poses)-1], self.poses[-1])
        self.path_len += translation_norm(delta_pose)
        trans_delta = translation_norm(delta_pose)
        # rot_delta = rotation_angle(delta_pose)
        rospy.logdebug('Odometry error at pos. %i: %.3f m (%.1f %%)', self.index,
                       trans_delta, 100. * trans_delta / self.path_len if self.path_len > 0. else 0.)

    def run(self, evt):
        self.slam_step()
        self.compute_localization_accuracy()
        self.publish_path()
        self.publish_gt_path()
        self.publish_cloud()
        # self.publish_camera_info()
        self.move()

    def publish_camera_info(self, distortion_model="plumb_bob"):
        camera_info = CameraInfo()
        camera_info.header.frame_id = self.camera_frame
        camera_info.header.stamp = rospy.Time.now()
        camera_info.width = self.current_pose.rgb_image.shape[-2]
        camera_info.height = self.current_pose.rgb_image.shape[-3]
        camera_info.K = self.current_pose.intrinsics.squeeze()[:3, :3].cpu().numpy().flatten().tolist()
        camera_info.distortion_model = distortion_model
        self.caminfo_pub.publish(camera_info)
        return camera_info

    def publish_path(self):
        assert self.current_pose.shape == (4, 4)
        pose_msg = matrix_to_pose_msg(self.current_pose.cpu().numpy())
        publish_tf_pose(pose_msg, child_frame_id=self.camera_frame)
        self.path.poses.append(pose_msg)
        self.path.header.stamp = rospy.Time.now()
        self.path_pub.publish(self.path)
        rospy.logdebug('Published path')

    def publish_gt_path(self, n_up_to=None):
        if not n_up_to:
            n_up_to = len(self.path.poses)
        pose_msg = matrix_to_pose_msg(self.poses_gt[n_up_to - 1].cpu().numpy())
        publish_tf_pose(pose_msg, child_frame_id=self.camera_frame+'_gt')
        self.path_gt.poses.append(pose_msg)
        self.path_gt.header.stamp = rospy.Time.now()
        self.path_gt_pub.publish(self.path_gt)
        rospy.logdebug('Published ground truth path')

    def publish_cloud(self):
        # publish point cloud
        n_pts = np.ceil(self.pointclouds.points_padded.shape[1] / self.map_step).astype(int)
        cloud = np.zeros((n_pts,), dtype=[('x', 'f4'), ('y', 'f4'), ('z', 'f4'),
                                          ('r', 'f4'), ('g', 'f4'), ('b', 'f4')])
        for i, f in enumerate(['x', 'y', 'z']):
            cloud[f] = self.pointclouds.points_padded[..., i].detach().squeeze().cpu().numpy()[::self.map_step]
        for i, f in enumerate(['r', 'g', 'b']):
            cloud[f] = self.pointclouds.colors_padded[..., i].detach().squeeze().cpu().numpy()[::self.map_step] / 255.
        pc_msg = msgify(PointCloud2, cloud)
        pc_msg.header.stamp = rospy.Time.now()
        pc_msg.header.frame_id = self.world_frame
        self.pc_pub.publish(pc_msg)
        rospy.logdebug('Published point cloud')


def main():
    rospy.init_node('supervised_depth_correction_ros', log_level=rospy.DEBUG)
    subseqs = [
        "2011_09_26_drive_0001_sync",
        "2011_09_26_drive_0009_sync",
        "2011_09_26_drive_0011_sync",
        "2011_09_26_drive_0018_sync",
        "2011_09_30_drive_0016_sync"
    ]
    # subseq = np.random.choice(subseqs, 1)[0]
    subseq = rospy.get_param('~kitti_sequence', subseqs[1])
    proc = GradslamROS(subseq)
    rospy.spin()


if __name__ == '__main__':
    main()
